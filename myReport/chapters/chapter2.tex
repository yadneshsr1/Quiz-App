\chapter{Literature Survey}
\section{Technology stacks for quiz systems (MEAN, MERN, LAMP)}

The appropriate selection of a technology stack is very crucial when it comes to building any web or mobile application. Every piece of software is built for a different purpose. Choosing the right technology for building any application depends on the intended purpose of the application. This can also be considered a definitive decision in the subsequent failure or success of an organisation, as it determines whether the company is able to satisfy the customer requirements in the future. Making an informed choice at the beginning of the development phase can significantly reduces and signs of further inconvenience.The best empirical benchmark is provided by Lei, Ma, and Tan [1], who compare PHP, Python, and Node.js using ApacheBench and scenario tests and consistently discover throughput/latency advantages for Node.js in I/O-bound workloads. The method is provided by Chhetri [4]: an event-driven, single-threaded loop on V8 reduces context switches and per-request threads, maintaining the high connection concurrency characteristic of real-time quiz backends. Nuance this image, Vilhelmsson [2] demonstrates that under CPU-heavy or database-intensive conditions, well-tuned multi-threaded servers (like Go/IIS) can outperform Node.js. The evidence foundation is expanded through comparative framings: Jaiswal and Shahi [5] map OS–language–database decisions that frequently explain disparities in maintainability and observed speed, while Bradach[and Garncarz[[3] drive MERN/MEAN/LAMP performance targets across the stack.Topuz, Saka, Fatsa, and Kurşun [11] list non-negotiable needs for assessment system design (abstracting from platform-specific pedagogy) that push architecture beyond sheer throughput, including authentication, lockdown/proctoring, monitoring, and analytics. Ayad's [12] and Wen, Yeh, and Huang's [13] LAMP exemplars show mature modules and safe, page-centric exam portals with robust relational guarantees. Al-Debagy and Martinek [14] demonstrate that monoliths can provide higher throughput at moderate concurrency at the deployment layer, while Node-based stacks scale horizontally through lightweight service frameworks and clustering/multiprocessing (Maatouki, Szuba, Meyer, and Streit [15]; Fysarakis, Spanoudakis, Tsigkanos, and Vlachos [16]). All things considered, the data favours MEAN/MERN for real-time, connection-heavy flows (WebSockets, streaming feedback) and LAMP for page-rendered delivery and transactional integrity—balancing concurrency model, benchmarked performance, and operational viability.

\setlength{\parskip}{1em} 

 \section{Framework performance and concurrency}

\setlength{\parskip}{1em} 

Lei, Ma, and Tan (2014) evaluate Node.js against PHP and Python-Web and come to the conclusion that it is "lightweight and efficient" for sites that require a lot of input and output [1]. As the execution model is expanded outward, Zhao, Xia, and Le (2013) demonstrate a WebSocket paradigm in which responsive, data-intensive real-time interactions are maintained via Node's event-driven, non-blocking design [3]. Dalbard's BSc thesis (2021) examines comparison experiments and indicates that Node.js outperforms alternatives on computational performance while demonstrating the best memory utilisation and CPU consumption [2], supporting these architecture-level assertions with system evidence. These threads work well together: native WS pipelines reduce latency, the event loop reduces per-request overhead, and effective resource utilisation maintains high concurrency. This alignment is ideal for quiz 
systems that have dashboards, live scoring, and bursty traffic. [1, 2], [3]

\setlength{\parskip}{1em}

 \section{HCI rules and micro-layout evidence for assessment UIs}

All things considered, Shneiderman's \cite{wikipediaShneiderman2025} corpus—most especially the Eight Golden Rules—provides a strict, empirical basis for creating consistent, feedback-rich, and error-preventive user interfaces. In terms of creating, taking, and marking quizzes, his universal usability attitude translates into accessibility-by-default (clear images, forgiving flows, multi-layered complexity). The focus on quick, reversible interactions (system status visibility, undo/escape, and unambiguous mappings) tackles well-known Blackboard issues such brittle "force completion" and hidden availability toggles. Last but not least, his human-centered AI viewpoint reinterprets automation as augmentation (trust, dependability, supervision), directing proctoring, analytics, and fair grading characteristics. When taken as a whole, these guidelines support the dissertation's UI/UX specifications and offer verifiable standards for a more dependable and usable quiz system.

\setlength{\parskip}{1em} 


\section{What "UX" really is (concept frameworks & AI-for)}

\setlength{\parskip}{1em} 

Stefan Hellweger and Xiaofeng Wang (2015) conducted a conceptual literature review study in "What is User Experience Really: towards a UX Conceptual Framework"\cite{hellweger2015what} in which they used manual snowballing to combine 114 UX terms from 21 sources into a single framework that is illustrated by smartphone diallers (Fig. 1; Sec. III; Sec. IV).  Design choices like the "+" international prefix and moving the "delete" control to lessen slips are revealed by empirical comparisons of the dial pads for iOS, Android, and Skype (Fig. 2); however, the results are constrained by single-rater subjectivity and the lack of inter-element analysis (Discussion/Conclusion).  In order to account for both pragmatic and hedonistic aspects of user experience, the study theoretically questions the adequacy of the user-product-interaction triangle and suggests three emerging layers: Impacting Factors, UX Characteristics, and Effects (Fig. 1; Sec. II–III). In their Double Diamond synthesis of 359 publications on AI-for-UX, Lu, Yang, Zhao, Zhang, and Li (2024)\cite{lu2024aiux} observe that while the field has grown rapidly since 2020, it has a tendency towards technology, with only 24.3\% of phase-mapped research using human-centered methodologies (Fig. 3, Fig. 5).
 They point out two fundamental flaws: there is no common standard for design quality and datasets that are concentrated on static screens (poor on cross-screen flows) (Table 1; §4.5).
 To properly represent UX work beyond images, they thus demand designer-centric statistics and assessment criteria (§4.5, §5.4).
 The conclusion emphasises that, from a Human-Centered AI perspective, AI should enhance rather than replace empathy-building (Abstract; §5.2).
\setlength{\parskip}{1em}

\section{Gamified quizzing : benefits, mechanisms, and fragilities}

 The tone is established by Wang & Tahir's 2020 systematic review\cite{WangTahir2020KahootReview} (k=93), which found that Kahoot! typically improves performance, energises classroom dynamics, and reduces fear in surveys, experiments, and quasi-experiments.
 However, the very scope that lends credibility to this analysis also reveals heterogeneity—tasks, metrics, and implementations differ sufficiently to impair the accuracy of causality and raise suspicions of publication bias.
 Zhang & Yu (2021)\cite{ZhangYu2021KahootEIT} map how results emerge through interaction and cooperation and validate advances, but they highlight a recurrent fragility: impacts depend on learner familiarity, orchestration, and context.
 Their construct maps guide readers towards social processes, such as peer presence, competitiveness, and quick feedback, but they also warn that ineffective task design or network frictions might negate advantages.
 According to classroom interviews by Licorish et al. (2018)\cite{Licorish2018RPTEL}, while time expenditures, scoreboard pressure, and excessive competitiveness might distract or demotivate certain students, attention, engagement, retention, and enjoyment all increase.
 They also demonstrate post-game conversation and anonymity as silent workhorses, allowing involvement without shame and turning estimates into long-lasting knowledge.
 The design levers of transparent deployment, instant feedback, and rich logging are validated by Daradoumis et al. (2019)\cite{Daradoumis2019CompEduc}, going beyond games, however their perception-centric evidence restricts causal assertions and leans towards programming settings.
 Johns (2015)\cite{Johns2015DKG} provides an inspirational story that fits the overall plot but has no inferential significance—it's more of a brochure than a standard.
 A common engine appears across the corpus: social presence, instant feedback loops, and gamified retrieval practice under time constraint. These elements are strong when well staged but fragile when hurried or misaligned.
 Wang and Tahir are ranked top for their scope and diverse designs, followed by Zhang and Yu for their interaction map and context-boundness, Licorish for their mechanistic depth and small-N/self-report, Daradoumis for their architectural understanding and constrained causality, and Johns for their background-only.
 
\setlength{\parskip}{1em} 
 "The Effects of Graphical Interface Design Characteristics on Human–Computer Interaction Task Efficiency" by Michalski, Grobelny, and Karwowski (2006)\cite{michalski2006effects}.  The study clearly demonstrates that compact square or horizontal, high-density layouts yield the fastest acquisition times, while small icons slow users and panel side (left vs. right) has no reliable effect. However, this conclusion is limited by the fact that it is based on a homogeneous lab sample with transient panels, which limits generalisability, and some unbalanced ANOVAs.  Underscoring layout and search over motor-only factors, their regression on size, horizontality, and dispersion explains ≈90.8\% of variance, significantly surpassing a Welford-form Fitts' fit (R²≈50.1\%).  Although the authors warn that actual per-task increases may be small, the theoretical justification links these benefits to foveal acuity limitations and left-to-right scanning patterns.
 
\setlength{\parskip}{1em}
\section{LMS comparisons (Moodle,    zBlackboard) and institutional fit}

 Patricia D. Simon et al.'s An Assessment of Learning Management System Use in Higher Education: Perspectives from a Comprehensive Sample of Teachers and Students (2025) \cite{simon2025assessment} and Priyavahani Subramanian et al.'s A Study of Comparison between Moodle and Blackboard based on Case Studies for Better LMS (2014)\cite{subramanian2014comparison} both look at the effectiveness and uptake of LMS platforms from different angles.  With an emphasis on communication, productivity, and student involvement tools, Subramanian et al. compare Moodle 2.0 and Blackboard 9.1 using a case-based approach. They find that Moodle is more flexible, cost-effective, and open-source adaptable than Blackboard, despite some complaints about feature limitations and user annoyances with interface design (pp. 29–33).Simon et al., on the other hand, use structured qualitative interviews with ten faculties in Hong Kong, placing their findings within the frameworks of TPACK, TAM, and LMS Usage. They show how Moodle changed from being a peripheral repository to a central platform during COVID-19, but they also point out that poor training, design complexity, and persistent underutilization of collaborative tools are the main obstacles (pp. 742–760).  While the two studies agree that user familiarity, training, and pedagogical integration are more important for LMS success than technological provision, they also highlight the differences between Moodle's open architecture and Blackboard's restricted modularity.  Together, they show how end-user perception and institutional context critically affect the theoretical potential of learning management systems (LMSs) for collaborative and active learning, highlighting constructivist foundations that are frequently compromised by low use of interactive features.